# Initial settings
verbose: true
y_classificaton_column: "GUI" # Column which contains the classification result
drop_columns: []
test_size: 0.3 # Percentage of data used for testing
debug_output_path: ""

# Preprocessing
stemming: false
lemmitization: true

# Word2Vec
vector_size: 500 # vector_size
window: 10 # window
w2v_epochs: 35
sequence_length: 50 # sequence_length

# Numerical data
numerical_data_shape: 4 # numerical_data_shape

# Neural network settings
neural_network:
  word_embedding:
    steps:
      - name: "Input-Word-Embedding"
        type: "Input"
        input_shape: 50 # sequence_length
      - name: "Embedding"
        type: "Embedding"
        input_shape: None # Len of vocabulary
        imput_length: 50 # sequence_length
        output_shape: 500 # vector_size
        weights: "embeddings"
        trainable: false
      - name: "LSTM-1"
        type: "LSTM"
        units: 50 # sequence_length
        dropout: 0.2
        return_sequences: true
      - name: "LSTM-2"
        type: "LSTM"
        units: 50 # sequence_length
        dropout: 0.2
        return_sequences: false
      - name: "Dense-1-WE"
        type: "Dense"
        units: 220
        activation: "relu"
  multi_layer_perception:
    steps:
      - name: "Input-Multi-Layer-Perceptron"
        type: "Input"
        input_shape: 4 # numerical_data_shape
      - name: "Dense-1-MLP"
        type: "Dense"
        units: 8
        activation: "relu"
      - name: "Dense-2-MLP"
        type: "Dense"
        units: 4
        activation: "relu"
  concatenate:
    steps:
      - name: "Dense-1"
        type: "Dense"
        units: 32
        activation: "relu"
  out:
    name: "Final-Dense"
    type: "Dense"
    units: 'data[config["y_classificaton_column"]].nunique()'
    activation: "softmax"
  optimizer:
    name: "adam"
    learning_rate: 0.001 # Unused
    decay: 0.0 # Unused
    momentum: 0.0 # Unused
    nesterov: false # Unused
    clipnorm: 0.0 # Unused
    clipvalue: 0.0 # Unused
    beta_1: 0.9 # Unused
    beta_2: 0.999 # Unused
    epsilon: 1e-08 # Unused
    amsgrad: false # Unused
  loss: "sparse_categorical_crossentropy"
  metrics: [] # Unused
  batch_size: 64
  epochs: 30
  shuffle: true